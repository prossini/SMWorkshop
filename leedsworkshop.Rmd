---
title: "Data Collection with Twitter API (V.1.1)"
author: "Patricia Rossini, University of Glasgow"
date: 'June 27-28, 2022'
output: 
  html_document:
    toc: true
---

```{r setup, include=FALSE, eval=FALSE}
knitr::opts_chunk$set(include = TRUE)

library(rtweet)

load("~/R/tweet-collections-credentialsR.RData")
token <- create_token(
  app = app_name, #your app name
  consumer_key = app_key, #consumer key for your app, replace the text between quotes with your app's key 
  consumer_secret = app_secret)

```


```{r data, include=FALSE}
load("leaders_tweets270622.RData")

```


## Getting Started

Before digging in, here are some basic commands in R that you need to familiarize yourself with. 
We will learn more as we progress/ You don't have to paste any of this in your script for this class, these are just pointers for general operations in R.

```{r, eval=FALSE, message=FALSE, inspect=FALSE}
# assign actions to an object
x <- "something"
# open csv files
y <- read.csv("path.csv")
#open RData files
load("file.RData")
# save as csv file
write.csv(df, "file.csv", row.names = F)
# save as RData file
save(df, file = "df.RData")
```

Note that there is a code for CSV files and another one for RData files. RData files are great to work in R because you can save multiple objects and open them all with just one command instead of loading a CSV per dataframe.

For some basic information and guidance about R and RStudio, check out these tutorials:

-[Getting started with RStudio](https://moderndive.netlify.app/1-getting-started.html)

-[R-Studio Basics by R-Ladies Sydney](https://rladiessydney.org/courses/ryouwithme/01-basicbasics-0/)


It is good practice to call your packages in the beginning of your script by using the library command. You may also want to set your work directory and a few options, such as removing scientific notations. The code below demonstrates how you can call a package in R. 


```{r calling packages,  message=FALSE, inspect=FALSE, eval=FALSE}
# options to remove scientific notations and limit digits in numeric outputs to 4
options(scipen=999, digits = 4)
# call required packages
library(rtweet)
library(tidyverse)
library(lubridate)
library(glue)
library(descr)
library(stringr)

```

## Connecting R to the Twitter API:

Twitter is among the easiest/most open social media platforms to collect data from. You can get data from Twitter using the API (application programming interface), which require creating a [developers account](http://dev.twitter.com). 

Now, you need to authenticate with Twitter using the credentials you created for your app. 

```{r twitter oauth, eval=FALSE, message=FALSE, inspect=FALSE}

## paste use your credentials here. 

## Create a token
token <- create_token(
  app = 'app_name', #your app name 
  consumer_key = "consumerKey", #consumer key for your app, replace the text between quotes with your app's key 
  consumer_secret = 'consumerSecret') #consumer secret for your app, replace the text between quotes with your app's secret 

```

### Timeline Search

Let's begin with timeline searches, that is, retrieving up to 3200 tweets from a single (or multiple) public accounts. 
You would use these commands to collect the content of tweets by an account, including data about engagement (likes, retweets).

For instance, let's compare tweets by Boris Johnson & the Conservative party vs tweets by Keir Starmer & Labour

```{r search API, eval=FALSE}

## getting tweets from one user and assigning them to an object named 'Boris'
## 
boris <- get_timeline("BorisJohnson", n=3200, retryOnRateLimit=120, resultType = "recent")


## getting tweets from several users and assigning them to a single object named 'leaders'
leaders <- get_timeline(c("BorisJohnson", "Keir_Starmer", "Conservatives", "UKLabour") , n=3200, retryOnRateLimit=120, resultType = "recent", parse = TRUE)

# if we want to save this data to work on it later, just use the following command: 
save(leaders, file = "leaders_tweets270622.RData") # to save it as an R object 
write_as_csv(leaders, file_name = "leaders_tweets270622.csv") # to save as a CSV that can be opened in excel. This function is from the package rtweet, not the same as 'write.csv' from base R. 
``` 

Note that I added the date I collected the data to the filename before saving. This is just for 'housekeeping': it is easier to remember when you created a data collection if you keep track of dates in filenames.


Now we have a dataframe with 90 variables and over 12,000 Tweets from the four accounts.  
The first step is to inspect our data by looking at the column names:

```{r checking the df}
colnames(leaders)
```


With this command, we can see the different columns in our dataframe. We have engagement data (e.g. likes, retweets), data about devices used to tweet, geolocation etc. You can check Twitter's [documentation](https://developer.twitter.com/en/docs/twitter-api/data-dictionary/object-model/tweet) to see what each of these columns mean if you are curious. 


We can also view our dataframe in RStudio by clickIng on it in the Environment tab, or inspect the first few columns using: 


```{r inspect, eval=FALSE, inspect=FALSE}
head(leaders)
```

We downloaded tweets from four different accounts, so it is useful to inspect the data. 
Let's look at how many tweets we have per account. It also makes sense to check the earliest date of each account's tweets so that we can filter our data afterwards to make sure we are comparing accounts in the same period of time. 


```{r descriptives, include=TRUE, eval=FALSE, inspect=FALSE}

freq(leaders$screen_name) # freq is a function of the package descr. Unsurprisingly, we have about 3200 for all accounts, as this is the maximum we can retrieve using the free API

# we can use dplyr to do the same:

leaders %>% group_by(screen_name) %>%
  count()


#let's check the dates. 
#First we convert the date column to a 'date' object using the function as.Date:
leaders$created_at <- as.Date(leaders$created_at)

#then, we check the earliest and latest tweet date per account:
leaders %>% group_by(screen_name) %>% summarise(min(created_at), max(created_at))

# from this we notice that tweets by Boris Johnson go way back to July, 2019, but all others were some time in 2020. 
# To make our dataset comparable, let's limit the analysis to tweets posted in 2021, filtering by date. 
# I will assign the filtered dataset to a new object just in case we want to return to the full set some other time. 

leaders_2021 <- leaders %>% filter(date >= "2021-01-01")
leaders_2021 <- leaders %>% filter(date >= "2021-01-01" & date < "2022-01-01")

leaders_2021 %>% group_by(screen_name) %>% summarise(min(created_at), max(created_at))
leaders_2021 %>% group_by(screen_name) %>% count()
# let's inspect to make sure it looks ok
leaders_analysis %>% group_by(screen_name) %>% summarise(min(created_at), max(created_at)) # great, now all our accounts have tweets for the same period of time! 


# with this code, we save our data to start off tomorrow from it. 
save(leaders, leaders_2021, file = "leaders270622.RData")
```



### Hashtags and searching terms


Now, a different type of data we can get is a sample of tweets about a topic or hashtag, using search terms and search operators. You can learn more about which different [search operators](https://developer.twitter.com/en/docs/twitter-api/v1/rules-and-filtering/search-operators) can be used on Twitter. 

In this example, let's make a simple search to collect tweets using a hasthag.

Bear in mind this is a sample of tweets (capped at 1% of all tweets at the moment of the query) and we don't have a way to know the 'population'--that is, how many people were actually using this hashtag at any given time. As such, any analysis or results would have to account for the limitation of using a sample of an unkrnown population. 


```{r search tweets, include=TRUE, eval=FALSE, inspect=FALSE}
hasthag <- search_tweets(
  "#sundayvibes",include_rts = FALSE, retryonratelimit = TRUE) # here we are asking the API for tweets using this hashtag, excluding RTs

# search terms using search operators (AND, OR)

rt <- search_tweets(q = "rstats AND python")
 
```

If we inspect this dataset (using colnames, or clicking on the environment), you will see that here we have tweets by many different accounts that used the hashtag.  


### Networks (friends, followers)

It is also possible to download someone's list of followers and 'friends' (i.e., people one follows). 
This can be useful to inspect networks and to conduct a social network analysis. 


```{r profiles, include=TRUE, eval=FALSE, inspect=FALSE}

# people I follow
i_fw <- get_friends("patyrossini")
# people following me
my_fw <- get_followers("patyrossini")

```


### Streaming API

The streaming API returns public posts on a 'stream', that is, posted from the time you begin the collection. This is normally used to track events in real time, such as hashtags, or the reactions of specific accounts to a given event. You can use four methods to build a stream query:
 - Sampling a small random sample of all publicly available tweets
 - Stream tweets based on a search-like query 
 - Stream tweets by certain users based on user ids
 - Stream tweets based on location using geo coordinates 
```{r}

```

```{r stream tweets, include=TRUE, eval=FALSE, inspect=FALSE}

# stream tweets based on search (can use advance operator)
st <- stream_tweets(q = "weather", timeout = 30, lang='en') ## added lang = en to make sure we only get tweets in english. can be used in any search query

# stream random sample
sr <- stream_tweets(q = "", timeout = 30)
 
```




